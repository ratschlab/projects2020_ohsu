{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851b2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from Bio import SeqIO\n",
    "import argparse\n",
    "import os \n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import timeit\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a620a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "search_res = '/cluster/work/grlab/projects/projects2020_OHSU/proteomics_fixMerge_25012024/tide_search_joint/TCGA-24-1431/tide-search-concat.txt'\n",
    "OHSU_index_mapping = '/cluster/work/grlab/projects/projects2020_OHSU/proteomics_fixMerge_25012024/OHSU/TCGA-24-1431/trypsine_digest/pepID_joint_original.tsv.gz'\n",
    "ETH_index_mapping = '/cluster/work/grlab/projects/projects2020_OHSU/proteomics_fixMerge_25012024/ETH/TCGA-24-1431/trypsine_digest/pepID_joint_original.tsv.gz'\n",
    "save_folder = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d2f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_result_rows(df_search):\n",
    "    id_to_row = defaultdict(list)\n",
    "    for i, idx in enumerate(df_search['protein id']):\n",
    "        if idx is np.nan:\n",
    "            print('ERROR: Search not successful on all fractions of sample. Please RERUN')\n",
    "        for name_ in idx.split(','):\n",
    "            if 'pepID' not in name_:\n",
    "                continue\n",
    "            pep_ix = int(name_.split('-')[1].replace('(1)', ''))\n",
    "            id_to_row[pep_ix].append(i)\n",
    "    return id_to_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69edb96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_search_result_simple(id_to_SearchRow, valid_pep_ids):\n",
    "    select_rows = set()\n",
    "    for id_ in valid_pep_ids:\n",
    "        select_rows.update(id_to_SearchRow[id_])\n",
    "    return select_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b3a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_pipeline(pipeline_index_mapping, joint_pepID_as_rows, search_res):\n",
    "\n",
    "    indexes_from_pipeline = set([int(i.split('-')[1]) for i in pipeline_index_mapping['reindex']])\n",
    "    indexes_joint_search = set(joint_pepID_as_rows.keys())\n",
    "    subset_pipeline_pepID = indexes_joint_search.intersection(indexes_from_pipeline)\n",
    "\n",
    "    print(f'Selected {len(subset_pipeline_pepID)} out of {len(indexes_joint_search)} peptide indexes searched')\n",
    "\n",
    "    subset_pipeline_rows = select_search_result_simple(joint_pepID_as_rows, subset_pipeline_pepID)\n",
    "\n",
    "    print(f'Selected {len(subset_pipeline_rows)} out of {len(search_res)} peptide rows searched')\n",
    "\n",
    "    search_res_pipeline = search_res.iloc[list(subset_pipeline_rows), :]\n",
    "    return search_res_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6559440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_protein_id(search_res_pipeline, pipeline_index_mapping):\n",
    "    # Generate a dictionary with peptide indexes and rows containing the given index\n",
    "    ETH_pepID_as_rows = search_result_rows(search_res_pipeline)\n",
    "\n",
    "    # Dictionary with the indexes from the joint pipeline file (reindexed) as keys\n",
    "    # and the original indexes from either the ETH or the OHSU pipeline as values\n",
    "\n",
    "    pipeline_index_mapping_dict = {}\n",
    "    for joint_index, original_index in zip(pipeline_index_mapping['reindex'], pipeline_index_mapping['fasta_index']): #faster as iterrows\n",
    "        pipeline_index_mapping_dict[int(joint_index.split('-')[1])] = int(original_index.split('-')[1])\n",
    "\n",
    "    # Replace the joint IDs by the pipeline-specific ones in the output table restricted to the pipeline\n",
    "    counter = 0\n",
    "    indexes_lookup = []\n",
    "    replacement_value = []\n",
    "    print(f'Iterating over {len(pipeline_index_mapping_dict)} peptideIDs')\n",
    "    for id_reindexed, original_index in pipeline_index_mapping_dict.items():\n",
    "        counter +=1\n",
    "        if counter % 5000 == 0:\n",
    "            print(f'...{counter}')\n",
    "        # Locate the protein IDs to replace    \n",
    "        ids_to_replace = search_res_pipeline.iloc[ETH_pepID_as_rows[id_reindexed]]['protein id']\n",
    "\n",
    "        # Generate the strings with the replaced IDs\n",
    "        ids_back_to_original = [pepID.replace(str(id_reindexed), str(original_index)) \n",
    "                                for pepID in ids_to_replace]\n",
    "        # Some tests\n",
    "        test_ids_back_to_original = [pepID.replace(str(id_reindexed) + '(1)', str(original_index) + '(1)') \n",
    "                                for pepID in ids_to_replace]\n",
    "        for replace_method1, replace_method2 in zip(ids_back_to_original, test_ids_back_to_original):\n",
    "            assert(replace_method1 == replace_method2)\n",
    "\n",
    "        # store\n",
    "        indexes_lookup.extend(list(ids_to_replace.index))\n",
    "        replacement_value.extend(ids_back_to_original)\n",
    "\n",
    "\n",
    "    #Replace the joint IDs by the pipeline-specific ones\n",
    "\n",
    "    new_ids = pd.DataFrame.from_dict({'index': indexes_lookup, 'protein id new': replacement_value})\n",
    "    search_res_pipeline = search_res_pipeline.reset_index()\n",
    "    print('Size before merge', search_res_pipeline.shape)\n",
    "    search_res_pipeline = search_res_pipeline.merge(new_ids, on = 'index', how = 'inner')\n",
    "    print('Size after merge', search_res_pipeline.shape)\n",
    "    search_res_pipeline = search_res_pipeline.rename({'protein id': 'joint id'}, axis = 1)\n",
    "    search_res_pipeline = search_res_pipeline.rename({'protein id new': 'protein id'}, axis = 1)\n",
    "    return search_res_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79328ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f36bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tide_pipeline_split(search_res, ETH_index_mapping, OHSU_index_mapping, save_folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86e335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading\n",
    "search_res = pd.read_csv(search_res, sep = '\\t')\n",
    "OHSU_index_mapping = pd.read_csv(OHSU_index_mapping, sep = '\\t')\n",
    "ETH_index_mapping = pd.read_csv(ETH_index_mapping, sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253ab1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a dictionary with peptide indexes and rows containing the given index\n",
    "joint_pepID_as_rows = search_result_rows(search_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b032f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting OHSU pipeline\n",
      "Selected 34974 out of 40708 peptide indexes searched\n",
      "Selected 5038239 out of 5910567 peptide rows searched\n",
      "Subsetting ETH pipeline\n",
      "Selected 8683 out of 40708 peptide indexes searched\n",
      "Selected 1334582 out of 5910567 peptide rows searched\n"
     ]
    }
   ],
   "source": [
    "print('Subsetting OHSU pipeline')\n",
    "search_res_OHSU = subset_pipeline(OHSU_index_mapping, joint_pepID_as_rows, search_res)\n",
    "print('Subsetting ETH pipeline')\n",
    "search_res_ETH = subset_pipeline(ETH_index_mapping, joint_pepID_as_rows, search_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1722d1f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing for ETH pipeline\n",
      "Iterating over 9212 peptideIDs\n",
      "...1000\n",
      "...2000\n",
      "...3000\n",
      "...4000\n",
      "...5000\n",
      "...6000\n",
      "...7000\n",
      "...8000\n",
      "...9000\n",
      "Size before merge (1334582, 22)\n",
      "Size after merge (1334582, 23)\n",
      "Replacing for OHSU pipeline\n",
      "Iterating over 38363 peptideIDs\n",
      "...1000\n",
      "...2000\n",
      "...3000\n",
      "...4000\n",
      "...5000\n",
      "...6000\n",
      "...7000\n",
      "...8000\n",
      "...9000\n",
      "...10000\n",
      "...11000\n",
      "...12000\n",
      "...13000\n",
      "...14000\n",
      "...15000\n",
      "...16000\n",
      "...17000\n",
      "...18000\n",
      "...19000\n",
      "...20000\n",
      "...21000\n",
      "...22000\n",
      "...23000\n",
      "...24000\n",
      "...25000\n",
      "...26000\n",
      "...27000\n",
      "...28000\n",
      "...29000\n",
      "...30000\n",
      "...31000\n",
      "...32000\n",
      "...33000\n",
      "...34000\n",
      "...35000\n",
      "...36000\n",
      "...37000\n",
      "...38000\n",
      "Size before merge (5038239, 22)\n",
      "Size after merge (5038239, 23)\n"
     ]
    }
   ],
   "source": [
    "print('Replacing for OHSU pipeline')\n",
    "search_res_OHSU = replace_protein_id(search_res_OHSU, OHSU_index_mapping)\n",
    "print('Replacing for ETH pipeline')\n",
    "search_res_ETH = replace_protein_id(search_res_ETH, ETH_index_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8850174",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'FDR_file'\n",
    "extension = '.tsv'\n",
    "search_res_OHSU.to_csv(os.path.join(save_folder, base_name + '_OHSU' + extension), sep = '\\t', index = None)\n",
    "search_res_ETH.to_csv(os.path.join(save_folder, base_name + '_ETH' + extension), sep = '\\t', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='separates a joint reindexed file into one file per pipeline with the original indexes')\n",
    "    parser.add_argument(\"--file-joint\", help='tide search file on union of pipelines to separate in two pipelines')\n",
    "    parser.add_argument(\"--map-eth-file\", help='file for eth containing the mapping table between original ids and shared ids')\n",
    "    parser.add_argument(\"--map-ohsu-file\",help='file for ohsu containing the mapping table between original ids and shared ids')\n",
    "    parser.add_argument(\"--save-folder\",help='base folder to save results')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    tide_pipeline_split(args.file_joint, args.map_eth_file, args.map_ohsu_file, args.save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89c0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE \n",
    "# Write the wrapper\n",
    "# Write the command line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
